{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPyEHtsn0b6x"
   },
   "source": [
    "# DECISION TREES AND MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_EO_W7l0b6y"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(transform_output = \"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kAWUAwYp0b64"
   },
   "source": [
    "A **decision tree** model is a flowchart-like structure in which each **internal node** represents a test on a feature, each **branch** represents a possible outcome of the test and each **leaf node** represents a target (probability distribution). The paths from root (top) to leaf (bottom) represent prediction rules. \n",
    "\n",
    "Decision trees can be used for both classification and regression tasks, but here we will restrict the description to classification trees. Here is an example of a decision tree:\n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"https://github.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/raw/master/notebooks/6_ensemble_learning/decisiontree.png\"/>\n",
    "</center>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-H3YsDbz0b7F"
   },
   "source": [
    "The first test is performed in the **root node** (top node) and checks if feature `X[9]` has a value smaller than or equal to -0.5350. If it has, then we follow the left branch in the flowchart, if it hasn't we follow the right branch. This process is repeated until we reach a leaf node from which we obtain predicted class probabilities. \n",
    "\n",
    "The decision tree is built in a **top-down** fashion where the main question is how to choose which feature to split at each node? The answer is to find the feature that best splits the target class into the purest possible children nodes. This **measure of purity** is typically computed as the opposite of the **entropy** (which is a measure of impurity) of the class label probabilities in the data set. \n",
    "\n",
    "The idea behind classification decision tree construction is to assign an **entropy value to a set of data label probabilities** and use this entropy to find the **best split** at each node in the decision tree. Suppose we have a data set with 100 samples of which 20 belong to class 1 and 80 belong to class 2. For this data set, the probability $p_1$ that a data point belongs to class 1 is 0.2, the probability $p_2$ that a data point belongs to class 2 is 0.8. \n",
    "\n",
    "The entropy $H$ of these two class label probabilities is computed as\n",
    "\n",
    "$$H(p_1,p_2) = -p_1 log_2(p_1) -p_2 log_2(p_2).$$ \n",
    "\n",
    "More general the entropy of any number of probabilities is computed as\n",
    "\n",
    "$$H(p_i) = -\\sum_i p_i log_2(p_i),$$\n",
    "\n",
    "where $i$ ranges over the class probabilities. Let's compute the entropy for the probability of the classes in our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ZoQr8w9w0b7F",
    "outputId": "6d4ca433-cc72-45f8-feff-4cac245748fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7219280948873623\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "print(entropy([.2,.8],base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBWPQJ_m0b7K"
   },
   "source": [
    "Now suppose our data set contains only one sample for class 1 and 99 for class 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "ZNXzgDHW0b7K",
    "outputId": "ac0a1e5c-2a42-4af5-94ba-b2928ccf6031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08079313589591118\n"
     ]
    }
   ],
   "source": [
    "print(entropy([.01,.99],base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0QXG2_00b7O"
   },
   "source": [
    "The entropy for this data set is much lower. To maximize the entropy of a data set we have to create one with an equal number of samples in class 1 and class 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "HJXvipe30b7O",
    "outputId": "c188033b-d612-4d5f-f8e8-ccc6818df4d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(entropy([.5,.5],base=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qy-QRd80b7R"
   },
   "source": [
    "In this case, the uncertainty about the classes is maximal. \n",
    "\n",
    "The **binary entropy function** for two classes looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "id": "17m-hEJd0b7S",
    "outputId": "4b53f8c4-86b1-406d-d9ca-e7dee2b2f35f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "pvals = np.linspace(0, 1)        \n",
    "plt.plot(pvals, [entropy([p,1-p]) for p in pvals])\n",
    "plt.xlabel(r'$p_1=1-p_2$')\n",
    "plt.ylabel('entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8WYyG8Yq0b7Z"
   },
   "source": [
    "A decision tree is constructed by **recursively splitting** (partitioning) a data set $D$. For each split the learning algorithm finds the feature that *best* partitioned the data set. To find this feature we can use several criteria, one of them is called **information gain**.\n",
    "\n",
    "Information gain is the difference between the current entropy of a system and the entropy measured after a feature and test is chosen. If $D$ is a set of samples and $x$ is a possible feature that partitions $D$ in subsets $D_v$, then:\n",
    "\n",
    "<br>\n",
    "$$G(D,x) = \\text{Entropy}(D) - \\sum\\frac{|D_v|}{|D|} \\text{Entropy}(D_v).$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs-yM7TP0b7a"
   },
   "source": [
    "The **decision tree learning algorithm** computes the information gain for each feature and test and selects the feature and test with the largest gain in information. This way, it searches the \"tree space\" according to a greedy strategy, i.e. not all possible trees are evaluated.\n",
    "\n",
    "Decision trees can become very complex, which makes them prone to overfitting the training set. To prevent this, the decision tree learning algorithm stops growing the tree if the information gain is not sufficient to justify the extra complexity of adding another node. Other **stopping criteria** exist such as limiting the number of samples in a node or limiting the depth of the tree.\n",
    "\n",
    "Let's load a data set that contains information about translation initiation sites (TIS) in genes [1]. The data set contains feature vectors that each represent a nucleotide sequence of length 203. The middle three nucleotides in these sequences are always ATG (i.e. the candidate TIS) and are labeled '1' (positive class) if they are known to form the start of a translation of a gene and '0' (negative class) otherwise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "25dkw4450b7b",
    "outputId": "31f4fd7e-c148-4976-ebef-229ed0fb1d6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UP_A</th>\n",
       "      <th>DOWN_A</th>\n",
       "      <th>UP_R</th>\n",
       "      <th>DOWN_R</th>\n",
       "      <th>UP_N</th>\n",
       "      <th>DOWN_N</th>\n",
       "      <th>UP_D</th>\n",
       "      <th>DOWN_D</th>\n",
       "      <th>UP_C</th>\n",
       "      <th>DOWN_C</th>\n",
       "      <th>...</th>\n",
       "      <th>DOWN_S</th>\n",
       "      <th>UP_T</th>\n",
       "      <th>DOWN_T</th>\n",
       "      <th>UP_W</th>\n",
       "      <th>DOWN_W</th>\n",
       "      <th>UP_Y</th>\n",
       "      <th>DOWN_Y</th>\n",
       "      <th>UP_V</th>\n",
       "      <th>DOWN_V</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   UP_A  DOWN_A  UP_R  DOWN_R  UP_N  DOWN_N  UP_D  DOWN_D  UP_C  DOWN_C  ...  \\\n",
       "0     2       2     4       2     0       2     0       4     0       1  ...   \n",
       "1     0       0     3       2     0       2     2       1     3       0  ...   \n",
       "2     6       2     5       1     0       2     0       2     1       0  ...   \n",
       "3     0       6     1       1     0       1     0       0     0       2  ...   \n",
       "4     2       3     3       2     0       1     1       2     2       0  ...   \n",
       "\n",
       "   DOWN_S  UP_T  DOWN_T  UP_W  DOWN_W  UP_Y  DOWN_Y  UP_V  DOWN_V  label  \n",
       "0       2     1       0     0       0     0       1     2       0      1  \n",
       "1       1     4       2     0       0     0       2     1       3      1  \n",
       "2       2     0       0     0       0     0       3     1       3      1  \n",
       "3       4     0       1     0       0     0       1     0       1      1  \n",
       "4       2     3       2     1       1     0       0     1       1      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"https://raw.githubusercontent.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/master/notebooks/6_ensemble_learning/TISs.csv\",sep=\";\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCayQqli0b7h"
   },
   "source": [
    "The features are straightforward. For instance the feature \"UP_A\" counts the number of codons coding for amino acid A that are found in the upstream region of the sequence (relative to the candidate TIS), while feature \"DOWN_V\" counts the number of codons coding for amino acid V that are found in the downstream region of the sequence. \n",
    "\n",
    "There are 40 features (plus one column for the label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "nS5g1DMk0b7i",
    "outputId": "02f34635-70ee-49f7-a4d3-d9d273c8855a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13375, 41)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjrqRSt20b7n"
   },
   "source": [
    "The data set is unbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "my8jBLyq0b7o",
    "outputId": "a1cfabac-6125-43a9-ee09-75c39dc3b751"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10063\n",
       "1     3312\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a decision tree model to the data and compute the accuracy of the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on dataset: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "X = dataset.loc[:,dataset.columns!=\"label\"]\n",
    "y = dataset.loc[:,\"label\"]\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X,y)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(\"accuracy on dataset: {:.2f}\".format(metrics.accuracy_score(y,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks like a good model!\n",
    "\n",
    "Decision trees have several advantages: \n",
    "\n",
    "* ease of interpretation\n",
    "* handles continuous and discrete features\n",
    "* invariant to monotone transformation of features\n",
    "* variable selection automated\n",
    "* low bias (for deep trees)\n",
    "\n",
    "The disadvantage is that decision trees are prone to overfitting. This is discussed in the folloing section.\n",
    "\n",
    "## model selection\n",
    "\n",
    "The accuracy on the dataset used for fitting (or **training**) the model (the **training set**) only tells part of the story. This was already discussed in the context of regression in the chapter about model regularization. \n",
    "\n",
    "What we really care about is how well our model will perform on unseen (not seen during training) data, i.e. the generalization performance.\n",
    "\n",
    "Let's split our data set randomly into a training set and a **test set**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set contains 30% of the feature vectors, the training set contains the remaining 70%.\n",
    "\n",
    "Let's fit a decision tree model on the training set and compute it's accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set: 1.00\n",
      "accuracy on test set: 0.77\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "\n",
    "predictions_train = model.predict(X_train)\n",
    "print(\"accuracy on train set: {:.2f}\".format(metrics.accuracy_score(y_train,predictions_train)))\n",
    "predictions_test = model.predict(X_test)\n",
    "print(\"accuracy on test set: {:.2f}\".format(metrics.accuracy_score(y_test,predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is a large difference between the training set and the test set accuracy. \n",
    "\n",
    "This is again due to overfitting the noise in the training set. Note that by creating a `DecisionTreeClassifier` model with default values for the hyperparameters, we allow for maximum complexity of the fitted decision tree. This is because the hyperparameter `max_depth`is set to `None` by default, which means the tree can be grown to maximum depth. \n",
    "\n",
    "Let's fix the complexity of the fitted decision tree to `max_depth = 3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set: 0.79\n",
      "accuracy on test set: 0.78\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "predictions_train = model.predict(X_train)\n",
    "print(\"accuracy on train set: {:.2f}\".format(metrics.accuracy_score(y_train,predictions_train)))\n",
    "predictions_test = model.predict(X_test)\n",
    "print(\"accuracy on test set: {:.2f}\".format(metrics.accuracy_score(y_test,predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the difference between training set and test accuracy becomes smaller, and the test set accuracy increases.\n",
    "\n",
    "Model selection is about the following considerations:\n",
    "\n",
    "- when is a model a good model (e.g. what metric to use to validate the generalization performance of the model)?\n",
    "- how to best pre-process the data (e.g. how to best normalize the data)?\n",
    "- what to assume about the true model underlying the data (e.g linear/non-linear)?\n",
    "- what are good values for the hyperparameters (this is known as **model tuning**)?\n",
    "\n",
    "In case of our decision tree model, we could train models with different values for `max_depth` and select the value that performs best on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal value for max_depth: 6 (accuracy on test set: 0.81)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_max_depth = -1\n",
    "\n",
    "to_plot = []\n",
    "for max_depth_ in range(2,20):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth_)\n",
    "    model.fit(X_train,y_train)\n",
    "    predictions_train = model.predict(X_train)\n",
    "    predictions_test = model.predict(X_test)\n",
    "    to_plot.append([max_depth_,metrics.accuracy_score(y_train,predictions_train),\"train\"])\n",
    "    accuracy_test = metrics.accuracy_score(y_test,predictions_test)\n",
    "    to_plot.append([max_depth_,accuracy_test,\"test\"])\n",
    "    if accuracy_test > best_accuracy:\n",
    "        best_accuracy = accuracy_test\n",
    "        best_max_depth = max_depth_\n",
    "\n",
    "print(\"optimal value for max_depth: {} (accuracy on test set: {:.2f})\".format(best_max_depth,best_accuracy))\n",
    "\n",
    "to_plot = pd.DataFrame(to_plot,columns=[\"max_depth\",\"accuracy\",\"data split\"])\n",
    "\n",
    "sns.lineplot(data=to_plot,x=\"max_depth\",y=\"accuracy\",hue=\"data split\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with this apporach. We have now used (or seen) the test set to select the best value for `max_depth`, i.e. the value that shows best performance on the test set. This might not be thebest value for truly unseen external data, and we did not evaluate this.\n",
    "\n",
    "To solve this, yet another part of the data set should be held out (known as the **validation set**). In this case the model is fitted on the training set and the optimal value for `max_depth` is selected using this validation set. A model is then trained on the union of the train and validation set using the best value for `max_depth` and the generalization performance of this model is then computed on the test set. \n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of data points used for training the model. Especially for smaller data sets this means that not only the fit of the model (training set) but also the decision on the values of the hyperparameters (validation set) as well as the estimation of the generalization performance (test set) will depend strongly on a particular random choice of the data set splits. \n",
    "\n",
    "To make our conclusions less dependent on the random data split we can perform **cross-validation** (CV). In the setup, called **$k$-fold CV**, the data set is randomly split into $k$ disjoint sets of approximately equal size. The following procedure is followed for each of the $k$ â€œfoldsâ€ $D_i$ ($i=1 \\ldots k$):\n",
    "\n",
    "1. train a model $m_i$ on the data points in folds $D_j$ with $j \\ne i$,\n",
    "2. use $m_i$ to compute predictions for $D_i.$\n",
    "\n",
    "As you can see we now have one prediction for each data point such that the model that computed that prediction was fitted on a train set that did not contain that data point. \n",
    "\n",
    "The performance measure reported by $k$-fold CV is then computed using these predictions and constitutes a good estimate of the generalization performance of the model. \n",
    "\n",
    "By choosing large enough values for $k$ (typical value is 10) the size of the training set becomes larger, while still computing a meaningful prediction for each data point. \n",
    "\n",
    "Instead of splitting the training set further into a validation set, we use $k$-fold CV to find the optimal value for `max_depth`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-fold CV accuracy: 0.77 (max_depth = 2)\n",
      "k-fold CV accuracy: 0.79 (max_depth = 3)\n",
      "k-fold CV accuracy: 0.80 (max_depth = 4)\n",
      "k-fold CV accuracy: 0.80 (max_depth = 5)\n",
      "k-fold CV accuracy: 0.81 (max_depth = 6)\n",
      "k-fold CV accuracy: 0.81 (max_depth = 7)\n",
      "k-fold CV accuracy: 0.81 (max_depth = 8)\n",
      "k-fold CV accuracy: 0.81 (max_depth = 9)\n",
      "k-fold CV accuracy: 0.81 (max_depth = 10)\n",
      "k-fold CV accuracy: 0.80 (max_depth = 11)\n",
      "k-fold CV accuracy: 0.80 (max_depth = 12)\n",
      "k-fold CV accuracy: 0.80 (max_depth = 13)\n",
      "k-fold CV accuracy: 0.80 (max_depth = 14)\n",
      "k-fold CV accuracy: 0.79 (max_depth = 15)\n",
      "k-fold CV accuracy: 0.79 (max_depth = 16)\n",
      "k-fold CV accuracy: 0.79 (max_depth = 17)\n",
      "k-fold CV accuracy: 0.79 (max_depth = 18)\n",
      "k-fold CV accuracy: 0.79 (max_depth = 19)\n",
      "optimal value for max_depth: 7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "best_accuracy = 0\n",
    "best_max_depth = -1\n",
    "for max_depth_ in  range(2,20):\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth_)\n",
    "    predictions_cv = cross_val_predict(model, X_train, y_train, method=\"predict\",cv=10)\n",
    "    accuracy_cv = metrics.accuracy_score(y_train,predictions_cv)\n",
    "    print(\"k-fold CV accuracy: {:.2f} (max_depth = {})\".format(accuracy_cv,max_depth_))\n",
    "    if accuracy_cv > best_accuracy:\n",
    "        best_accuracy = accuracy_cv\n",
    "        best_max_depth = max_depth_\n",
    "print(\"optimal value for max_depth: {}\".format(best_max_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the optimal value for `max_depth` (`best_max_depth`) as obtained during $k$-fold CV to fit a model on the full training set and estimate it's generalization performance on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set: 0.86\n",
      "accuracy on test set: 0.79\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(max_depth=best_max_depth)\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "predictions_train = model.predict(X_train)\n",
    "print(\"accuracy on train set: {:.2f}\".format(metrics.accuracy_score(y_train,predictions_train)))\n",
    "predictions_test = model.predict(X_test)\n",
    "print(\"accuracy on test set: {:.2f}\".format(metrics.accuracy_score(y_test,predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8R1TV0z30b7s"
   },
   "source": [
    "Scikit-learn has a method called `GridSearchCV` that implements this procedure more efficiently.\n",
    "\n",
    "We need to first create Python dictionary the defines the hyperparameters we want to optimize and the values to be consideren. In our case this was hyperparameter `max_depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {'max_depth': range(2,20)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can run `GridSearchCV` to evaluate all values using $k$-fold CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "RKE6VL-A0b7t",
    "outputId": "2bec1016-2b1c-41a8-de00-aa6c02dc7ce9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: range(2, 20)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: range(2, 20)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n",
       "             param_grid={'max_depth': range(2, 20)})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=search_space, cv=10)\n",
    "\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model fitted on the full training set using the optimal value for our hyperparameter can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=7)\n",
      "0.809334094993113\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use that model to make predictions on unseen external data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set: 0.79\n"
     ]
    }
   ],
   "source": [
    "predictions_test = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"accuracy on test set: {:.2f}\".format(metrics.accuracy_score(y_test,predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `GridSearchCV` we can find optimal values for a combination of hyperparameters. Note that all possible combinations will be evaluated, which can take long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=7)\n",
      "0.8097607612949128\n",
      "accuracy on test set: 0.79\n"
     ]
    }
   ],
   "source": [
    "search_space = {'max_depth': range(2,20),\n",
    "                'min_samples_split': [2,20,200]}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid=search_space, cv=10)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "predictions_test = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"accuracy on test set: {:.2f}\".format(metrics.accuracy_score(y_test,predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details about the hyperparameter optimization results can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019226</td>\n",
       "      <td>0.007706</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.778015</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.767094</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.768162</td>\n",
       "      <td>0.774086</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018938</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 20}</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.778015</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.767094</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.768162</td>\n",
       "      <td>0.774086</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.008176</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 2, 'min_samples_split': 200}</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.778015</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.767094</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.768162</td>\n",
       "      <td>0.774086</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021985</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>0.004687</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
       "      <td>0.789755</td>\n",
       "      <td>0.792956</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.787865</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022038</td>\n",
       "      <td>0.009959</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 20}</td>\n",
       "      <td>0.789755</td>\n",
       "      <td>0.792956</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.787865</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022078</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.006008</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 200}</td>\n",
       "      <td>0.789755</td>\n",
       "      <td>0.792956</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.787865</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.031778</td>\n",
       "      <td>0.012446</td>\n",
       "      <td>0.006106</td>\n",
       "      <td>0.007493</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 4, 'min_samples_split': 2}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.800362</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.030456</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.005772</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 4, 'min_samples_split': 20}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.800362</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.038150</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 4, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.800362</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.042550</td>\n",
       "      <td>0.012145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 2}</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.803629</td>\n",
       "      <td>0.818376</td>\n",
       "      <td>0.787393</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.800469</td>\n",
       "      <td>0.012827</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.037027</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 20}</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.818376</td>\n",
       "      <td>0.787393</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.800896</td>\n",
       "      <td>0.012995</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.030641</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 5, 'min_samples_split': 200}</td>\n",
       "      <td>0.811099</td>\n",
       "      <td>0.803629</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.774573</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.800362</td>\n",
       "      <td>0.015767</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.037795</td>\n",
       "      <td>0.010423</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 6, 'min_samples_split': 2}</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.814301</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.809015</td>\n",
       "      <td>0.011774</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.037895</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.004688</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 6, 'min_samples_split': 20}</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.811099</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.826923</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.808054</td>\n",
       "      <td>0.011804</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.049031</td>\n",
       "      <td>0.012679</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.004658</td>\n",
       "      <td>6</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 6, 'min_samples_split': 200}</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.817503</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.806023</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.045323</td>\n",
       "      <td>0.015344</td>\n",
       "      <td>0.006370</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 2}</td>\n",
       "      <td>0.814301</td>\n",
       "      <td>0.824973</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.809761</td>\n",
       "      <td>0.009893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050322</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 20}</td>\n",
       "      <td>0.814301</td>\n",
       "      <td>0.823906</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.809440</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.011803</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 7, 'min_samples_split': 200}</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.821581</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.818376</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.806023</td>\n",
       "      <td>0.012187</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.063554</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.004097</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 8, 'min_samples_split': 2}</td>\n",
       "      <td>0.815368</td>\n",
       "      <td>0.810032</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.808480</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.054597</td>\n",
       "      <td>0.012995</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 8, 'min_samples_split': 20}</td>\n",
       "      <td>0.815368</td>\n",
       "      <td>0.811099</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.806624</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.804954</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.057987</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 8, 'min_samples_split': 200}</td>\n",
       "      <td>0.805763</td>\n",
       "      <td>0.816435</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.818376</td>\n",
       "      <td>0.778846</td>\n",
       "      <td>0.805489</td>\n",
       "      <td>0.012147</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.070860</td>\n",
       "      <td>0.012761</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.006245</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 2}</td>\n",
       "      <td>0.813234</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.821581</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.806624</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.772436</td>\n",
       "      <td>0.806984</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.054216</td>\n",
       "      <td>0.013413</td>\n",
       "      <td>0.008799</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 20}</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.802819</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.014005</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 200}</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.811099</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.818376</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804422</td>\n",
       "      <td>0.012118</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.082257</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2}</td>\n",
       "      <td>0.800427</td>\n",
       "      <td>0.815368</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.822650</td>\n",
       "      <td>0.815171</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.767094</td>\n",
       "      <td>0.807840</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.095048</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 20}</td>\n",
       "      <td>0.803629</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.802285</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.058115</td>\n",
       "      <td>0.014007</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.814301</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.817308</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804528</td>\n",
       "      <td>0.012238</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.095453</td>\n",
       "      <td>0.017219</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 2}</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.811099</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.808761</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.814103</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.757479</td>\n",
       "      <td>0.803673</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.092876</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 20}</td>\n",
       "      <td>0.810032</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.799400</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.066569</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>11</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 11, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.813234</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804315</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.098582</td>\n",
       "      <td>0.025311</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 2}</td>\n",
       "      <td>0.815368</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.768162</td>\n",
       "      <td>0.804420</td>\n",
       "      <td>0.013358</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.086425</td>\n",
       "      <td>0.013155</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 20}</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.800427</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.764957</td>\n",
       "      <td>0.797370</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.068860</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>12</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.813234</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.813034</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804315</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.105559</td>\n",
       "      <td>0.034026</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 2}</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.808965</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.785256</td>\n",
       "      <td>0.806624</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.755342</td>\n",
       "      <td>0.796195</td>\n",
       "      <td>0.015687</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.088324</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 20}</td>\n",
       "      <td>0.810032</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.796088</td>\n",
       "      <td>0.012777</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.056488</td>\n",
       "      <td>0.013686</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>13</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 13, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.810032</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.803888</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.089724</td>\n",
       "      <td>0.020738</td>\n",
       "      <td>0.004703</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 14, 'min_samples_split': 2}</td>\n",
       "      <td>0.805763</td>\n",
       "      <td>0.811099</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.821581</td>\n",
       "      <td>0.789530</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.797797</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.094523</td>\n",
       "      <td>0.019892</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.007082</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 14, 'min_samples_split': 20}</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.799360</td>\n",
       "      <td>0.789530</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.787393</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.762821</td>\n",
       "      <td>0.793311</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.058165</td>\n",
       "      <td>0.013455</td>\n",
       "      <td>0.009507</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 14, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804101</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.111582</td>\n",
       "      <td>0.026069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 2}</td>\n",
       "      <td>0.797225</td>\n",
       "      <td>0.805763</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.779915</td>\n",
       "      <td>0.755342</td>\n",
       "      <td>0.793632</td>\n",
       "      <td>0.014985</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.095617</td>\n",
       "      <td>0.022493</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 20}</td>\n",
       "      <td>0.805763</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.800214</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.758547</td>\n",
       "      <td>0.792350</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.068498</td>\n",
       "      <td>0.016025</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.808965</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.803781</td>\n",
       "      <td>0.011718</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.100529</td>\n",
       "      <td>0.026507</td>\n",
       "      <td>0.006274</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_split': 2}</td>\n",
       "      <td>0.807898</td>\n",
       "      <td>0.796158</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.783120</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.791923</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.104797</td>\n",
       "      <td>0.019237</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_split': 20}</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.800427</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.791495</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.073955</td>\n",
       "      <td>0.017398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.808965</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.803781</td>\n",
       "      <td>0.011718</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.087184</td>\n",
       "      <td>0.012554</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 2}</td>\n",
       "      <td>0.789755</td>\n",
       "      <td>0.795091</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.810897</td>\n",
       "      <td>0.787393</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.787393</td>\n",
       "      <td>0.751068</td>\n",
       "      <td>0.789681</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.093743</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 20}</td>\n",
       "      <td>0.805763</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.758547</td>\n",
       "      <td>0.793632</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.064987</td>\n",
       "      <td>0.014704</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>17</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 17, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804101</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.117860</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 18, 'min_samples_split': 2}</td>\n",
       "      <td>0.799360</td>\n",
       "      <td>0.795091</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.806624</td>\n",
       "      <td>0.776709</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.797009</td>\n",
       "      <td>0.774573</td>\n",
       "      <td>0.751068</td>\n",
       "      <td>0.787757</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.094255</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 18, 'min_samples_split': 20}</td>\n",
       "      <td>0.802561</td>\n",
       "      <td>0.798292</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.758547</td>\n",
       "      <td>0.792243</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.073568</td>\n",
       "      <td>0.010308</td>\n",
       "      <td>0.003505</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>18</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 18, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.812166</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.804101</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.106718</td>\n",
       "      <td>0.026315</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 2}</td>\n",
       "      <td>0.794023</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.802350</td>\n",
       "      <td>0.780983</td>\n",
       "      <td>0.788462</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.784188</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.788398</td>\n",
       "      <td>0.014436</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.099440</td>\n",
       "      <td>0.020295</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 20}</td>\n",
       "      <td>0.804696</td>\n",
       "      <td>0.799360</td>\n",
       "      <td>0.787393</td>\n",
       "      <td>0.793803</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.795940</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.757479</td>\n",
       "      <td>0.791923</td>\n",
       "      <td>0.012546</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.065922</td>\n",
       "      <td>0.020505</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>19</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 19, 'min_samples_split': 200}</td>\n",
       "      <td>0.801494</td>\n",
       "      <td>0.808965</td>\n",
       "      <td>0.801282</td>\n",
       "      <td>0.798077</td>\n",
       "      <td>0.792735</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.809829</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.803781</td>\n",
       "      <td>0.011718</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.019226      0.007706         0.001014        0.003042   \n",
       "1        0.018938      0.007507         0.001565        0.004694   \n",
       "2        0.025140      0.008176         0.001562        0.004685   \n",
       "3        0.021985      0.007595         0.004687        0.007160   \n",
       "4        0.022038      0.009959         0.001563        0.004688   \n",
       "5        0.022078      0.007956         0.003117        0.006008   \n",
       "6        0.031778      0.012446         0.006106        0.007493   \n",
       "7        0.030456      0.010989         0.002144        0.005772   \n",
       "8        0.038150      0.009615         0.002418        0.004778   \n",
       "9        0.042550      0.012145         0.000000        0.000000   \n",
       "10       0.037027      0.019762         0.005488        0.006590   \n",
       "11       0.030641      0.007185         0.002342        0.005002   \n",
       "12       0.037795      0.010423         0.003128        0.006256   \n",
       "13       0.037895      0.010660         0.001563        0.004688   \n",
       "14       0.049031      0.012679         0.001553        0.004658   \n",
       "15       0.045323      0.015344         0.006370        0.007808   \n",
       "16       0.050322      0.012941         0.003125        0.006250   \n",
       "17       0.056848      0.011803         0.002191        0.004659   \n",
       "18       0.063554      0.008789         0.001874        0.004097   \n",
       "19       0.054597      0.012995         0.003682        0.006113   \n",
       "20       0.057987      0.011885         0.005044        0.007718   \n",
       "21       0.070860      0.012761         0.003123        0.006245   \n",
       "22       0.054216      0.013413         0.008799        0.007311   \n",
       "23       0.051961      0.014005         0.001564        0.004692   \n",
       "24       0.082257      0.019894         0.004953        0.006434   \n",
       "25       0.095048      0.017341         0.001968        0.004699   \n",
       "26       0.058115      0.014007         0.006554        0.008052   \n",
       "27       0.095453      0.017219         0.000703        0.002109   \n",
       "28       0.092876      0.017466         0.000262        0.000787   \n",
       "29       0.066569      0.011839         0.000306        0.000621   \n",
       "30       0.098582      0.025311         0.000096        0.000287   \n",
       "31       0.086425      0.013155         0.003115        0.006230   \n",
       "32       0.068860      0.021458         0.001563        0.004689   \n",
       "33       0.105559      0.034026         0.001565        0.004694   \n",
       "34       0.088324      0.013547         0.003017        0.005266   \n",
       "35       0.056488      0.013686         0.000277        0.000832   \n",
       "36       0.089724      0.020738         0.004703        0.007189   \n",
       "37       0.094523      0.019892         0.004635        0.007082   \n",
       "38       0.058165      0.013455         0.009507        0.007627   \n",
       "39       0.111582      0.026069         0.000000        0.000000   \n",
       "40       0.095617      0.022493         0.002033        0.004850   \n",
       "41       0.068498      0.016025         0.004373        0.006781   \n",
       "42       0.100529      0.026507         0.006274        0.007684   \n",
       "43       0.104797      0.019237         0.002503        0.005183   \n",
       "44       0.073955      0.017398         0.000000        0.000000   \n",
       "45       0.087184      0.012554         0.002426        0.005112   \n",
       "46       0.093743      0.019060         0.001104        0.003312   \n",
       "47       0.064987      0.014704         0.001565        0.004694   \n",
       "48       0.117860      0.016900         0.000261        0.000782   \n",
       "49       0.094255      0.010078         0.003559        0.006176   \n",
       "50       0.073568      0.010308         0.003505        0.006579   \n",
       "51       0.106718      0.026315         0.001744        0.004871   \n",
       "52       0.099440      0.020295         0.003118        0.006235   \n",
       "53       0.065922      0.020505         0.002448        0.005118   \n",
       "\n",
       "   param_max_depth param_min_samples_split  \\\n",
       "0                2                       2   \n",
       "1                2                      20   \n",
       "2                2                     200   \n",
       "3                3                       2   \n",
       "4                3                      20   \n",
       "5                3                     200   \n",
       "6                4                       2   \n",
       "7                4                      20   \n",
       "8                4                     200   \n",
       "9                5                       2   \n",
       "10               5                      20   \n",
       "11               5                     200   \n",
       "12               6                       2   \n",
       "13               6                      20   \n",
       "14               6                     200   \n",
       "15               7                       2   \n",
       "16               7                      20   \n",
       "17               7                     200   \n",
       "18               8                       2   \n",
       "19               8                      20   \n",
       "20               8                     200   \n",
       "21               9                       2   \n",
       "22               9                      20   \n",
       "23               9                     200   \n",
       "24              10                       2   \n",
       "25              10                      20   \n",
       "26              10                     200   \n",
       "27              11                       2   \n",
       "28              11                      20   \n",
       "29              11                     200   \n",
       "30              12                       2   \n",
       "31              12                      20   \n",
       "32              12                     200   \n",
       "33              13                       2   \n",
       "34              13                      20   \n",
       "35              13                     200   \n",
       "36              14                       2   \n",
       "37              14                      20   \n",
       "38              14                     200   \n",
       "39              15                       2   \n",
       "40              15                      20   \n",
       "41              15                     200   \n",
       "42              16                       2   \n",
       "43              16                      20   \n",
       "44              16                     200   \n",
       "45              17                       2   \n",
       "46              17                      20   \n",
       "47              17                     200   \n",
       "48              18                       2   \n",
       "49              18                      20   \n",
       "50              18                     200   \n",
       "51              19                       2   \n",
       "52              19                      20   \n",
       "53              19                     200   \n",
       "\n",
       "                                         params  split0_test_score  \\\n",
       "0      {'max_depth': 2, 'min_samples_split': 2}           0.779082   \n",
       "1     {'max_depth': 2, 'min_samples_split': 20}           0.779082   \n",
       "2    {'max_depth': 2, 'min_samples_split': 200}           0.779082   \n",
       "3      {'max_depth': 3, 'min_samples_split': 2}           0.789755   \n",
       "4     {'max_depth': 3, 'min_samples_split': 20}           0.789755   \n",
       "5    {'max_depth': 3, 'min_samples_split': 200}           0.789755   \n",
       "6      {'max_depth': 4, 'min_samples_split': 2}           0.801494   \n",
       "7     {'max_depth': 4, 'min_samples_split': 20}           0.801494   \n",
       "8    {'max_depth': 4, 'min_samples_split': 200}           0.801494   \n",
       "9      {'max_depth': 5, 'min_samples_split': 2}           0.802561   \n",
       "10    {'max_depth': 5, 'min_samples_split': 20}           0.802561   \n",
       "11   {'max_depth': 5, 'min_samples_split': 200}           0.811099   \n",
       "12     {'max_depth': 6, 'min_samples_split': 2}           0.806830   \n",
       "13    {'max_depth': 6, 'min_samples_split': 20}           0.806830   \n",
       "14   {'max_depth': 6, 'min_samples_split': 200}           0.806830   \n",
       "15     {'max_depth': 7, 'min_samples_split': 2}           0.814301   \n",
       "16    {'max_depth': 7, 'min_samples_split': 20}           0.814301   \n",
       "17   {'max_depth': 7, 'min_samples_split': 200}           0.807898   \n",
       "18     {'max_depth': 8, 'min_samples_split': 2}           0.815368   \n",
       "19    {'max_depth': 8, 'min_samples_split': 20}           0.815368   \n",
       "20   {'max_depth': 8, 'min_samples_split': 200}           0.805763   \n",
       "21     {'max_depth': 9, 'min_samples_split': 2}           0.813234   \n",
       "22    {'max_depth': 9, 'min_samples_split': 20}           0.807898   \n",
       "23   {'max_depth': 9, 'min_samples_split': 200}           0.802561   \n",
       "24    {'max_depth': 10, 'min_samples_split': 2}           0.800427   \n",
       "25   {'max_depth': 10, 'min_samples_split': 20}           0.803629   \n",
       "26  {'max_depth': 10, 'min_samples_split': 200}           0.801494   \n",
       "27    {'max_depth': 11, 'min_samples_split': 2}           0.806830   \n",
       "28   {'max_depth': 11, 'min_samples_split': 20}           0.810032   \n",
       "29  {'max_depth': 11, 'min_samples_split': 200}           0.801494   \n",
       "30    {'max_depth': 12, 'min_samples_split': 2}           0.815368   \n",
       "31   {'max_depth': 12, 'min_samples_split': 20}           0.812166   \n",
       "32  {'max_depth': 12, 'min_samples_split': 200}           0.801494   \n",
       "33    {'max_depth': 13, 'min_samples_split': 2}           0.806830   \n",
       "34   {'max_depth': 13, 'min_samples_split': 20}           0.810032   \n",
       "35  {'max_depth': 13, 'min_samples_split': 200}           0.801494   \n",
       "36    {'max_depth': 14, 'min_samples_split': 2}           0.805763   \n",
       "37   {'max_depth': 14, 'min_samples_split': 20}           0.807898   \n",
       "38  {'max_depth': 14, 'min_samples_split': 200}           0.801494   \n",
       "39    {'max_depth': 15, 'min_samples_split': 2}           0.797225   \n",
       "40   {'max_depth': 15, 'min_samples_split': 20}           0.805763   \n",
       "41  {'max_depth': 15, 'min_samples_split': 200}           0.801494   \n",
       "42    {'max_depth': 16, 'min_samples_split': 2}           0.807898   \n",
       "43   {'max_depth': 16, 'min_samples_split': 20}           0.806830   \n",
       "44  {'max_depth': 16, 'min_samples_split': 200}           0.801494   \n",
       "45    {'max_depth': 17, 'min_samples_split': 2}           0.789755   \n",
       "46   {'max_depth': 17, 'min_samples_split': 20}           0.805763   \n",
       "47  {'max_depth': 17, 'min_samples_split': 200}           0.801494   \n",
       "48    {'max_depth': 18, 'min_samples_split': 2}           0.799360   \n",
       "49   {'max_depth': 18, 'min_samples_split': 20}           0.802561   \n",
       "50  {'max_depth': 18, 'min_samples_split': 200}           0.801494   \n",
       "51    {'max_depth': 19, 'min_samples_split': 2}           0.794023   \n",
       "52   {'max_depth': 19, 'min_samples_split': 20}           0.804696   \n",
       "53  {'max_depth': 19, 'min_samples_split': 200}           0.801494   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.778015           0.769231           0.767094   \n",
       "1            0.778015           0.769231           0.767094   \n",
       "2            0.778015           0.769231           0.767094   \n",
       "3            0.792956           0.784188           0.785256   \n",
       "4            0.792956           0.784188           0.785256   \n",
       "5            0.792956           0.784188           0.785256   \n",
       "6            0.806830           0.798077           0.794872   \n",
       "7            0.806830           0.798077           0.794872   \n",
       "8            0.806830           0.798077           0.794872   \n",
       "9            0.803629           0.818376           0.787393   \n",
       "10           0.807898           0.818376           0.787393   \n",
       "11           0.803629           0.814103           0.786325   \n",
       "12           0.814301           0.803419           0.817308   \n",
       "13           0.811099           0.801282           0.817308   \n",
       "14           0.817503           0.803419           0.800214   \n",
       "15           0.824973           0.804487           0.810897   \n",
       "16           0.823906           0.801282           0.807692   \n",
       "17           0.812166           0.801282           0.803419   \n",
       "18           0.810032           0.808761           0.817308   \n",
       "19           0.811099           0.798077           0.809829   \n",
       "20           0.816435           0.801282           0.798077   \n",
       "21           0.812166           0.810897           0.821581   \n",
       "22           0.806830           0.804487           0.808761   \n",
       "23           0.811099           0.801282           0.798077   \n",
       "24           0.815368           0.809829           0.822650   \n",
       "25           0.812166           0.807692           0.805556   \n",
       "26           0.814301           0.801282           0.798077   \n",
       "27           0.811099           0.816239           0.807692   \n",
       "28           0.807898           0.807692           0.805556   \n",
       "29           0.813234           0.801282           0.798077   \n",
       "30           0.812166           0.811966           0.813034   \n",
       "31           0.800427           0.799145           0.799145   \n",
       "32           0.813234           0.801282           0.798077   \n",
       "33           0.808965           0.801282           0.807692   \n",
       "34           0.802561           0.798077           0.795940   \n",
       "35           0.810032           0.801282           0.798077   \n",
       "36           0.811099           0.801282           0.821581   \n",
       "37           0.799360           0.789530           0.792735   \n",
       "38           0.812166           0.801282           0.798077   \n",
       "39           0.805763           0.804487           0.809829   \n",
       "40           0.801494           0.790598           0.790598   \n",
       "41           0.808965           0.801282           0.798077   \n",
       "42           0.796158           0.795940           0.810897   \n",
       "43           0.800427           0.788462           0.792735   \n",
       "44           0.808965           0.801282           0.798077   \n",
       "45           0.795091           0.794872           0.810897   \n",
       "46           0.802561           0.791667           0.791667   \n",
       "47           0.812166           0.801282           0.798077   \n",
       "48           0.795091           0.794872           0.806624   \n",
       "49           0.798292           0.784188           0.794872   \n",
       "50           0.812166           0.801282           0.798077   \n",
       "51           0.801494           0.799145           0.802350   \n",
       "52           0.799360           0.787393           0.793803   \n",
       "53           0.808965           0.801282           0.798077   \n",
       "\n",
       "    split4_test_score  split5_test_score  split6_test_score  \\\n",
       "0            0.763889           0.778846           0.775641   \n",
       "1            0.763889           0.778846           0.775641   \n",
       "2            0.763889           0.778846           0.775641   \n",
       "3            0.778846           0.792735           0.786325   \n",
       "4            0.778846           0.792735           0.786325   \n",
       "5            0.778846           0.792735           0.786325   \n",
       "6            0.784188           0.805556           0.809829   \n",
       "7            0.784188           0.805556           0.809829   \n",
       "8            0.784188           0.805556           0.809829   \n",
       "9            0.779915           0.813034           0.808761   \n",
       "10           0.779915           0.813034           0.808761   \n",
       "11           0.774573           0.819444           0.805556   \n",
       "12           0.798077           0.817308           0.826923   \n",
       "13           0.797009           0.817308           0.826923   \n",
       "14           0.788462           0.820513           0.815171   \n",
       "15           0.801282           0.814103           0.813034   \n",
       "16           0.802350           0.814103           0.814103   \n",
       "17           0.793803           0.821581           0.813034   \n",
       "18           0.800214           0.815171           0.814103   \n",
       "19           0.799145           0.803419           0.813034   \n",
       "20           0.793803           0.819444           0.813034   \n",
       "21           0.810897           0.806624           0.817308   \n",
       "22           0.803419           0.804487           0.811966   \n",
       "23           0.792735           0.819444           0.813034   \n",
       "24           0.815171           0.809829           0.824786   \n",
       "25           0.813034           0.800214           0.811966   \n",
       "26           0.792735           0.819444           0.813034   \n",
       "27           0.810897           0.808761           0.805556   \n",
       "28           0.810897           0.793803           0.797009   \n",
       "29           0.792735           0.819444           0.813034   \n",
       "30           0.807692           0.804487           0.811966   \n",
       "31           0.810897           0.797009           0.797009   \n",
       "32           0.792735           0.819444           0.813034   \n",
       "33           0.797009           0.804487           0.785256   \n",
       "34           0.804487           0.791667           0.799145   \n",
       "35           0.792735           0.819444           0.811966   \n",
       "36           0.789530           0.800214           0.801282   \n",
       "37           0.800214           0.787393           0.799145   \n",
       "38           0.792735           0.819444           0.811966   \n",
       "39           0.797009           0.795940           0.791667   \n",
       "40           0.799145           0.790598           0.794872   \n",
       "41           0.792735           0.819444           0.811966   \n",
       "42           0.791667           0.783120           0.797009   \n",
       "43           0.802350           0.786325           0.793803   \n",
       "44           0.792735           0.819444           0.811966   \n",
       "45           0.787393           0.788462           0.795940   \n",
       "46           0.804487           0.788462           0.798077   \n",
       "47           0.792735           0.819444           0.811966   \n",
       "48           0.776709           0.791667           0.790598   \n",
       "49           0.801282           0.790598           0.799145   \n",
       "50           0.792735           0.819444           0.811966   \n",
       "51           0.780983           0.788462           0.790598   \n",
       "52           0.801282           0.790598           0.795940   \n",
       "53           0.792735           0.819444           0.811966   \n",
       "\n",
       "    split7_test_score  split8_test_score  split9_test_score  mean_test_score  \\\n",
       "0            0.779915           0.780983           0.768162         0.774086   \n",
       "1            0.779915           0.780983           0.768162         0.774086   \n",
       "2            0.779915           0.780983           0.768162         0.774086   \n",
       "3            0.790598           0.792735           0.785256         0.787865   \n",
       "4            0.790598           0.792735           0.785256         0.787865   \n",
       "5            0.790598           0.792735           0.785256         0.787865   \n",
       "6            0.804487           0.817308           0.780983         0.800362   \n",
       "7            0.804487           0.817308           0.780983         0.800362   \n",
       "8            0.804487           0.817308           0.780983         0.800362   \n",
       "9            0.794872           0.813034           0.783120         0.800469   \n",
       "10           0.794872           0.813034           0.783120         0.800896   \n",
       "11           0.795940           0.817308           0.775641         0.800362   \n",
       "12           0.811966           0.811966           0.782051         0.809015   \n",
       "13           0.810897           0.809829           0.782051         0.808054   \n",
       "14           0.808761           0.819444           0.779915         0.806023   \n",
       "15           0.816239           0.811966           0.786325         0.809761   \n",
       "16           0.817308           0.813034           0.786325         0.809440   \n",
       "17           0.810897           0.818376           0.777778         0.806023   \n",
       "18           0.808761           0.811966           0.783120         0.808480   \n",
       "19           0.808761           0.806624           0.784188         0.804954   \n",
       "20           0.809829           0.818376           0.778846         0.805489   \n",
       "21           0.808761           0.795940           0.772436         0.806984   \n",
       "22           0.811966           0.799145           0.769231         0.802819   \n",
       "23           0.809829           0.818376           0.777778         0.804422   \n",
       "24           0.801282           0.811966           0.767094         0.807840   \n",
       "25           0.808761           0.795940           0.763889         0.802285   \n",
       "26           0.809829           0.817308           0.777778         0.804528   \n",
       "27           0.814103           0.798077           0.757479         0.803673   \n",
       "28           0.803419           0.797009           0.760684         0.799400   \n",
       "29           0.809829           0.816239           0.777778         0.804315   \n",
       "30           0.804487           0.794872           0.768162         0.804420   \n",
       "31           0.802350           0.790598           0.764957         0.797370   \n",
       "32           0.809829           0.816239           0.777778         0.804315   \n",
       "33           0.806624           0.788462           0.755342         0.796195   \n",
       "34           0.802350           0.795940           0.760684         0.796088   \n",
       "35           0.809829           0.816239           0.777778         0.803888   \n",
       "36           0.803419           0.783120           0.760684         0.797797   \n",
       "37           0.802350           0.791667           0.762821         0.793311   \n",
       "38           0.809829           0.816239           0.777778         0.804101   \n",
       "39           0.799145           0.779915           0.755342         0.793632   \n",
       "40           0.800214           0.791667           0.758547         0.792350   \n",
       "41           0.809829           0.816239           0.777778         0.803781   \n",
       "42           0.793803           0.782051           0.760684         0.791923   \n",
       "43           0.799145           0.784188           0.760684         0.791495   \n",
       "44           0.809829           0.816239           0.777778         0.803781   \n",
       "45           0.795940           0.787393           0.751068         0.789681   \n",
       "46           0.802350           0.792735           0.758547         0.793632   \n",
       "47           0.809829           0.816239           0.777778         0.804101   \n",
       "48           0.797009           0.774573           0.751068         0.787757   \n",
       "49           0.804487           0.788462           0.758547         0.792243   \n",
       "50           0.809829           0.816239           0.777778         0.804101   \n",
       "51           0.792735           0.784188           0.750000         0.788398   \n",
       "52           0.798077           0.790598           0.757479         0.791923   \n",
       "53           0.809829           0.816239           0.777778         0.803781   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "0         0.005988               52  \n",
       "1         0.005988               52  \n",
       "2         0.005988               52  \n",
       "3         0.004419               48  \n",
       "4         0.004419               48  \n",
       "5         0.004419               48  \n",
       "6         0.010660               29  \n",
       "7         0.010660               29  \n",
       "8         0.010660               29  \n",
       "9         0.012827               28  \n",
       "10        0.012995               27  \n",
       "11        0.015767               32  \n",
       "12        0.011774                3  \n",
       "13        0.011804                5  \n",
       "14        0.012823                9  \n",
       "15        0.009893                1  \n",
       "16        0.010027                2  \n",
       "17        0.012187                8  \n",
       "18        0.009633                4  \n",
       "19        0.008759               11  \n",
       "20        0.012147               10  \n",
       "21        0.013172                7  \n",
       "22        0.011799               25  \n",
       "23        0.012118               13  \n",
       "24        0.015492                6  \n",
       "25        0.013818               26  \n",
       "26        0.012238               12  \n",
       "27        0.016108               24  \n",
       "28        0.014078               33  \n",
       "29        0.012048               15  \n",
       "30        0.013358               14  \n",
       "31        0.012406               35  \n",
       "32        0.012048               15  \n",
       "33        0.015687               36  \n",
       "34        0.012777               37  \n",
       "35        0.011770               20  \n",
       "36        0.015903               34  \n",
       "37        0.011810               40  \n",
       "38        0.011898               17  \n",
       "39        0.014985               38  \n",
       "40        0.012370               41  \n",
       "41        0.011718               21  \n",
       "42        0.013526               43  \n",
       "43        0.012397               45  \n",
       "44        0.011718               21  \n",
       "45        0.014449               46  \n",
       "46        0.013064               39  \n",
       "47        0.011898               17  \n",
       "48        0.015316               51  \n",
       "49        0.012832               42  \n",
       "50        0.011898               17  \n",
       "51        0.014436               47  \n",
       "52        0.012546               43  \n",
       "53        0.011718               21  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxd0HFls0b7z"
   },
   "source": [
    "**Reference**\n",
    "\n",
    "[1] Pedersen AG, Nielsen H. Proceedings 5th International Conference on Intelligent Systems for Molecular Biology. 1997. Neural network prediction of translation initiation sites in eukaryotes: perspectives for EST and genome analysis; pp. 226â€“233."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6a_Decision trees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
