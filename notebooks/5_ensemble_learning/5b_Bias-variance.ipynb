{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDYetE1s4FG2"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "import matplotlib.pyplot as plt;\n",
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import seaborn as sns;\n",
    "sns.set_context(\"notebook\", font_scale=1.2);\n",
    "sns.set_style(\"whitegrid\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYXSmAOY4FGz"
   },
   "source": [
    "# 5b. Bias / variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsG9jpJs4FG_"
   },
   "source": [
    "So far we have discussed several supervised machine learning algorithms that each learn a predictive model from a train set using a certain model representation and a certain objective function. Machine learning theory has shown that the prediction error such a model makes can be broken down into three parts: **the bias, the variance, and the irreducible error**.\n",
    "\n",
    "To explain these concepts we need to first understand that the train set used to learn a model is typically a very small subset of all possible data points. This means that the generalization performance (i.e. prediction error on unseen test set) depends, to a certain extend, on the particular choice of the train set sample (which is something we donâ€™t control). \n",
    "\n",
    "Imagine a dartboard where the goal is to hit the bullseye in the middle of the board. Now we throw several darts that try to hit the bullseye. The figure below shows several possible scenarios. \n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"https://github.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/raw/master/notebooks/6_ensemble_learning/bias-and-variance.jpg\" />\n",
    "</center>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_0YkB_Fi4FHC"
   },
   "source": [
    "In our context, the bullseye represents the correct value we want to predict. Each dart is a predictive model, trained on a different train set sample, that tries to predict the correct value.\n",
    " \n",
    "**The error due to bias** is the difference between the expected prediction of our model and the correct value that we try to predict. By expected prediction we mean the average prediction of models trained on different train set samples. In the figure above we can see that for low bias models the average (center point of the dart locations) is very close to the correct value. These models are able to predict the correct value. High bias models are not able to predict the correct value for any of the train set samples.\n",
    "\n",
    "**The error due to variance** is the variability of a model prediction for a given data point. The variance does not care about the correct value that we try to predict but instead looks at the variance of the prediction for different train set samples. In the figure above we can see that low variance models tend to hit the same (or very close) location for each train set sample, while predictions of high variance models are much more dependant on the actual choice of the train set sample.\n",
    "\n",
    "**The irreducible error** is a stochastic error that we cannot reduce by choosing a better model and is typically due to randomness or natural variability in a system. For instance, when describing a protein sequence by a feature vector to train a model that predicts the function of a protein based on these features then the randomness (or unknown) comes from the environment (or context) in which a protein acts that can also influence the function of a protein. \n",
    "\n",
    "Examples of low variance high bias algorithms are linear regression and logistic regression. They assume a relatively simple (not very flexible) model representation and as such are known to possess **low complexity**.\n",
    "\n",
    "Examples of high variance low bias algorithms are SVMs with Gaussian kernel and decision tree learning algorithms. They have a much more flexible model representation and as such are known to possess **high complexity**.\n",
    "\n",
    "This means that there is a **tradeoff** between the bias of a model and its variance. When minimizing one, the other will increase and *vice versa*. This is true because a model cannot possess low complexity and high complexity at the same time.  \n",
    "\n",
    "This tradeoff can be visualized as follows: \n",
    "\n",
    "<br/>\n",
    "<center>\n",
    "<img src=\"https://github.com/sdgroeve/Machine_Learning_course_UGent_D012554_data/raw/master/notebooks/6_ensemble_learning/biasvariance.png\" />\n",
    "</center>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mI3f1fz34FHH"
   },
   "source": [
    "The x-axis represents the complexity of a model. The y-axis shows the prediction error. As the model complexity increases the bias is reduced (the model is more flexible) but the variance will increase. The optimum model complexity is obtained when the increase in variance is equivalent to the reduction in bias.\n",
    "\n",
    "Note that the regularization term introduced for linear and logistic regression (as well as for the SVM) follows the same principle. High regularization means decreasing the complexity of the model representation by constraining the model parameters. This leads to higher bias and lower variance. Similarly, low regularization means increasing model complexity and results in lower bias with higher variance."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "6b_Bias-variance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
